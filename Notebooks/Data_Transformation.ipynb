{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf37996-2bc5-4b60-8ce4-19dd4e37c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fraud_detection_project import logger\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6c461-163b-4ecb-9d87-aa5808098300",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2086c4-d8cb-46dc-8603-72b4a5df12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29a70b-e127-4d4a-9ca7-f861c95b1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fraud_detection_project.constants import *\n",
    "from fraud_detection_project.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed9690-b3c2-4846-8183-f4462c470ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075cf7b-a19d-43d0-9929-bdc97781b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce47c0-202c-4fa4-a3ab-78100b2c0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "def download_from_gdrive(file_id: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Downloads a file from Google Drive using its file ID.\n",
    "    \"\"\"\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    gdown.download(url, output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33186c94-3e47-4840-9bbc-d1cdfa1f38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], format='%d-%m-%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eecffd-2fd8-492e-bef1-8acc6d025eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformation:\n",
    "\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def read_data(data_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        It reads a CSV file and returns a Pandas DataFrame\n",
    "\n",
    "        Args:\n",
    "          file_path (str): The path to the file you want to read.\n",
    "\n",
    "        Returns:\n",
    "          A dataframe\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"reading the Dataset\")\n",
    "            return pd.read_csv(data_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(e)\n",
    "        \n",
    "\n",
    "    def extract_date_features(self, data_path):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            self.data: pd.DataFrame = DataTransformation.read_data(data_path)\n",
    "            logger.info(\"Deriving the Date features from Transactions Date\")\n",
    "            # Convert the date column to datetime format\n",
    "            self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time'], format='%d-%m-%Y')\n",
    "            \n",
    "            # Extract month, year, and quarter\n",
    "            self.data['trans_month'] = self.data['trans_date_trans_time'].dt.month\n",
    "            self.data['trans_year'] = self.data['trans_date_trans_time'].dt.year\n",
    "            self.data['trans_quarter'] = self.data['trans_date_trans_time'].dt.quarter\n",
    "            \n",
    "            logger.info(\"Derived the new Date features from Transactions Date\")\n",
    "\n",
    "            return self.data \n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error:\", e)\n",
    "\n",
    "\n",
    "    def calculate_customer_age(self, data_path):\n",
    "       \n",
    "       try:\n",
    "            \n",
    "            self.data = self.extract_date_features(data_path)\n",
    "            \n",
    "            logger.info(\"Creating the Customer Age variable using DOB and Transactions Date\")\n",
    "            \n",
    "            self.data['dob'] = pd.to_datetime(self.data['dob'], format='%d-%m-%Y')\n",
    "            self.data['Cust_age'] = (self.data['trans_date_trans_time'] - self.data['dob']).dt.days // 365\n",
    "            \n",
    "            logger.info(\"Created the Customer Age variable\")\n",
    "\n",
    "            return self.data\n",
    "\n",
    "       except Exception as e:\n",
    "            raise Exception(\"Error:\", e)\n",
    "                        \n",
    "\n",
    "    def create_city_population_bins(self, data_path):\n",
    "        try:\n",
    "            \n",
    "            self.data = self.calculate_customer_age(data_path)\n",
    "\n",
    "            logger.info(\"Creating the city populations category by binning the city population\")\n",
    "\n",
    "            bins = [0, 5000, 50000, float('inf')]\n",
    "            labels = ['rural', 'sub-urban', 'urban']\n",
    "        \n",
    "            self.data['city_pop_category'] = pd.cut(self.data['city_pop'], bins=bins, labels=labels)\n",
    "\n",
    "            logger.info(\"Created the city_pop_category\")\n",
    "\n",
    "            return self.data\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error:\", e)\n",
    "        \n",
    "\n",
    "    def calculate_average_amount_by_category(self, data_path):\n",
    "\n",
    "        try:\n",
    "            \n",
    "            self.data = self.create_city_population_bins(data_path)\n",
    "\n",
    "            logger.info(\"Creating the avg_amount_by_category variable\")\n",
    "\n",
    "            avg_amount_by_category = self.data.groupby('category')['amt'].mean()\n",
    "            self.data['avg_amount_by_category'] = self.data['category'].map(avg_amount_by_category)\n",
    "            self.data = self.data.drop(labels=[\"trans_date_trans_time\", 'dob'], axis=1)\n",
    "            return self.data\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)\n",
    "    \n",
    "\n",
    "\n",
    "    def label_encoding(self, data_path):\n",
    "        try:\n",
    "            \n",
    "            self.data = self.calculate_average_amount_by_category(data_path)\n",
    "\n",
    "            logger.info(\"performing the Label encoding on Category and city_pop_category\")\n",
    "\n",
    "            encoded_data = self.data.copy()\n",
    "            columns = ['category', 'city_pop_category']\n",
    "            for col in columns:\n",
    "                encoder = LabelEncoder()\n",
    "                encoded_data[col] = encoder.fit_transform(self.data[col])\n",
    "            return encoded_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)\n",
    "        \n",
    "    def one_hot_encode_columns(self, data_path):\n",
    "        try:\n",
    "           \n",
    "            self.data = self.label_encoding(data_path)\n",
    "\n",
    "            logger.info(\"performing the one hot encoding on gender and trans_year\")\n",
    "            \n",
    "            cols = ['gender', 'trans_year']\n",
    "            encoded_data = self.data.copy()\n",
    "            encoded_data = pd.get_dummies(encoded_data, columns=cols, dtype=int)\n",
    "\n",
    "            logger.info(\"Done with feature encoding\")\n",
    "            \n",
    "            return encoded_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)\n",
    "\n",
    "\n",
    "    def fit_transform(self, data_path):\n",
    "        try:\n",
    "            \n",
    "            self.data = self.one_hot_encode_columns(data_path)\n",
    "\n",
    "            logger.info(\"performing feature scaling\")\n",
    "\n",
    "            columns_to_scale = ['amt', 'Cust_age', 'city_pop', 'avg_amount_by_category']\n",
    "            scaler = StandardScaler()\n",
    "            scaled_data = self.data.copy()\n",
    "            scaled_data[columns_to_scale] = scaler.fit_transform(self.data[columns_to_scale])\n",
    "\n",
    "            logger.info(\"Feature Scaling process is done\")\n",
    "        \n",
    "            return scaled_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)\n",
    "\n",
    "    def final_data(self, data_path):\n",
    "        try:\n",
    "            \n",
    "            self.data = self.fit_transform(data_path)\n",
    "\n",
    "            logger.info(\"Splitting Dataset into training and test sets Started\")\n",
    "            \n",
    "            # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "            train, test = train_test_split(self.data, test_size=0.2, random_state=42)\n",
    "\n",
    "            train.to_csv(local_csv_path.parent / \"train.csv\", index=False)\n",
    "            test.to_csv(local_csv_path.parent / \"test.csv\", index=False)\n",
    "\n",
    "\n",
    "            logger.info(\"Completed the split and stored Splited data into training and test sets\")\n",
    "            logger.info(train.shape)\n",
    "            logger.info(test.shape)\n",
    "\n",
    "            print(train.shape)\n",
    "            print(test.shape)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35b49d-7c30-49f4-bc2c-82102f7afee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "\n",
    "    gdrive_file_id = '1JRNRK-Iv5jD3aqf9F6Pn-eirYZreGbsi'  # from your YAML\n",
    "    local_csv_path = data_transformation_config.data_path\n",
    "    local_csv_path.parent.mkdir(parents=True, exist_ok=True)  # create folders if not exist\n",
    "\n",
    "    if not local_csv_path.exists():\n",
    "        url = f'https://drive.google.com/uc?id={gdrive_file_id}'\n",
    "        gdown.download(url, str(local_csv_path), quiet=False)\n",
    "    else:\n",
    "        print(f\"{local_csv_path} already exists. Skipping download.\")\n",
    "    \n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.final_data(local_csv_path)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
